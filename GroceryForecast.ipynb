{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicalities of AutoML forecasting\n",
    "\n",
    "In this notebook, we will walk through the process of trying to actually push a real world dataset through AutoML. AutoML has expectations of clean data which are rarely encountered with real world data. We will show various techniques of detecting the problems before they crash AutoML, and workarounds that will get you through the day.\n",
    "\n",
    "Eventually, these tips and tricks will become part of the AutoML product, prettied up and made more robust.\n",
    "\n",
    "Many of the cells in this notebook will show the first attempt at doing something, which is later abandoned or cleaned up. They are marked with `DEAD END`. Don't run them, they are there so that you can learn from my mistakes, not your.\n",
    "\n",
    "To begin, let's load some data. Our first dead end occurs in the data loading, and is shown in the companion notebook `GetDataFromSQL`. But for now, _assume_ we have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesDate</th>\n",
       "      <th>Item</th>\n",
       "      <th>Site</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1262885</td>\n",
       "      <td>1.262885e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2057</td>\n",
       "      <td>348</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>A53634</td>\n",
       "      <td>82DC</td>\n",
       "      <td>811B</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>996</td>\n",
       "      <td>43416</td>\n",
       "      <td>359820</td>\n",
       "      <td>391703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.201671e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.587775e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.928000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SalesDate     Item     Site  Channel      Quantity\n",
       "count      1262885  1262885  1262885  1262885  1.262885e+06\n",
       "unique        2057      348       11        9           NaN\n",
       "top     2017-05-26   A53634     82DC     811B           NaN\n",
       "freq           996    43416   359820   391703           NaN\n",
       "mean           NaN      NaN      NaN      NaN  1.201671e+01\n",
       "std            NaN      NaN      NaN      NaN  2.587775e+01\n",
       "min            NaN      NaN      NaN      NaN  1.000000e+00\n",
       "25%            NaN      NaN      NaN      NaN  5.000000e+00\n",
       "50%            NaN      NaN      NaN      NaN  8.000000e+00\n",
       "75%            NaN      NaN      NaN      NaN  1.200000e+01\n",
       "max            NaN      NaN      NaN      NaN  7.928000e+03"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "SQLdata = pandas.read_csv('data/tutorial_data.csv.bz2')\n",
    "SQLdata.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a friendly 1.2M rows, which is because it only contains the 348 items beginning with 'A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          A00968\n",
       "377        A010AB\n",
       "10431      A01370\n",
       "10463      A02987\n",
       "10778      A04247\n",
       "10798      A04A18\n",
       "16493      A04F8F\n",
       "16583      A05356\n",
       "18630      A05358\n",
       "18806      A05794\n",
       "20960      A06133\n",
       "22241      A0627A\n",
       "29656      A074CC\n",
       "29702      A0802D\n",
       "31102      A08952\n",
       "31398      A08B54\n",
       "32303      A09216\n",
       "32341      A094CE\n",
       "70554      A09781\n",
       "70654      A09D61\n",
       "71098      A0BC16\n",
       "78938      A0BCF4\n",
       "78998      A0BE42\n",
       "79053      A0BF66\n",
       "79469      A0CA5B\n",
       "81591      A0CD71\n",
       "84503      A0E454\n",
       "86679      A0E9E8\n",
       "86693      A0F431\n",
       "86813      A0FA4E\n",
       "            ...  \n",
       "1093802    AE79B6\n",
       "1093859    AE7C41\n",
       "1100816    AE815D\n",
       "1101168    AE8349\n",
       "1103239    AE91A8\n",
       "1103242    AE92B6\n",
       "1103813    AEA5F7\n",
       "1104110    AEBBB7\n",
       "1106332    AEC7F3\n",
       "1107898    AEDF2A\n",
       "1107954    AEE6C5\n",
       "1108399    AEF12C\n",
       "1109364    AF0B7F\n",
       "1109893    AF387A\n",
       "1139718    AF3B02\n",
       "1147859    AF5276\n",
       "1147928    AF56E5\n",
       "1148442    AF59E4\n",
       "1148509    AF6FB8\n",
       "1149349    AF7091\n",
       "1149505    AF7676\n",
       "1149621    AF88F0\n",
       "1150516    AF8A0C\n",
       "1178951    AF9EAB\n",
       "1180540    AF9FF9\n",
       "1194157    AFB98C\n",
       "1203356    AFC3A9\n",
       "1203424    AFCB88\n",
       "1238781    AFD153\n",
       "1238825    AFE443\n",
       "Name: Item, Length: 348, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQLdata['Item'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll establish the time series metadata. These say: we want to forecast the Quantity, along the time axis given by SalesDate, for each combination of Item, Site, and Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows pulled : 1262885\n",
      "Distinct time series : 6139\n"
     ]
    }
   ],
   "source": [
    "# How many series do we have?\n",
    "guppy = SQLdata.groupby(grain_colnames)\n",
    "\n",
    "print(\"Rows pulled : \" + str(len(SQLdata)))\n",
    "print(\"Distinct time series : \" + str(len(guppy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's forecast!\n",
    "\n",
    "AutoML in theory does not need much more than a data frame. So let's see how far AutoML gets on this **tough** dataset. Let us not even split into train and test (because we may need to re-work the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = SQLdata.copy()\n",
    "y_train = X_train.pop(target_colname).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin our forecasting journey with the boilerplate AutoML imports and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the experiment in the workspace\n",
    "# 'Experiment' roughly corresponds to a 'problem to solve'\n",
    "experiment_name = 'automl-grocery'\n",
    "# where to write models and other artifacts locally\n",
    "project_folder = './sample_projects/automl-grocery'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>westus2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Project Directory</th>\n",
       "      <td>./sample_projects/automl-grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>automl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run History Name</th>\n",
       "      <td>automl-grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDK version</th>\n",
       "      <td>0.1.0.3119914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription ID</th>\n",
       "      <td>938fa533-eeb9-4121-b97f-05b31c6eb088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Workspace</th>\n",
       "      <td>automl-customers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       \n",
       "Location           westus2                             \n",
       "Project Directory  ./sample_projects/automl-grocery    \n",
       "Resource Group     automl                              \n",
       "Run History Name   automl-grocery                      \n",
       "SDK version        0.1.0.3119914                       \n",
       "Subscription ID    938fa533-eeb9-4121-b97f-05b31c6eb088\n",
       "Workspace          automl-customers                    "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what our forecasting problem is, put the spec in the config object form, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n"
     ]
    },
    {
     "ename": "DataException",
     "evalue": "Frequencies can not be inferred for grain [('A00968', '33A1', '08BB')]. Please consider dropping that grain.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\training_utilities.py\u001b[0m in \u001b[0;36m_check_timeseries_input_and_get_tsdf\u001b[1;34m(X, y, x_raw_column_names, automl_settings, min_points, is_validation_data)\u001b[0m\n\u001b[0;32m    783\u001b[0m                         _check_grain_min_points(\n\u001b[1;32m--> 784\u001b[1;33m                             data_points, min_points, automl_settings, grain_names=grain_name_str)\n\u001b[0m\u001b[0;32m    785\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_validation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\training_utilities.py\u001b[0m in \u001b[0;36m_check_grain_min_points\u001b[1;34m(data_points, min_points, automl_settings, grain_names)\u001b[0m\n\u001b[0;32m    720\u001b[0m                     \u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrain_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m                     \u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_cross_validations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m                 )\n",
      "\u001b[1;31mDataException\u001b[0m: The data provided is insufficient for training grain: [A00968,33A1,08BB] for a valid training with cv 2, max_horizon 31 lags 0 and rolling window size 0. The current grain has only 8 points. Please consider reducing max_horizon, n_cross_validations or , lags, rolling window size or dropping that particular grain.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDataException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c9ca9af65a8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                              **time_series_settings)\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mlocal_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoml_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\core\\experiment.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0msubmit_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_experiment_submit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submit config {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\automlconfig.py\u001b[0m in \u001b[0;36m_automl_static_submit\u001b[1;34m(automl_config_object, workspace, experiment_name, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'show_output'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoml_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mtracking_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoml_config_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, run_configuration, compute_target, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, data, label, columns, cv_splits_indices, show_output, existing_run)\u001b[0m\n\u001b[0;32m    299\u001b[0m             self._fit_local(X=X, y=y, sample_weight=sample_weight, X_valid=X_valid, y_valid=y_valid,\n\u001b[0;32m    300\u001b[0m                             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                             existing_run=existing_run, sample_weight_valid=sample_weight_valid)\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_onnx_compatible_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_fit_local\u001b[1;34m(self, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, data, label, columns, cv_splits_indices, existing_run)\u001b[0m\n\u001b[0;32m    536\u001b[0m                                 \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m                                 \u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexisting_run\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 538\u001b[1;33m                                 sample_weight_valid=sample_weight_valid)\n\u001b[0m\u001b[0;32m    539\u001b[0m         \u001b[1;31m# Init the onnx converter with the original X.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[0mpkg_ver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_package_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_create_parent_run\u001b[1;34m(self, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, data, label, columns, cv_splits_indices, existing_run)\u001b[0m\n\u001b[0;32m    424\u001b[0m                                                                   logger=self.logger, is_adb_run=True)\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[0mtraining_utilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_training_data_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[0mtraining_utilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_blacklist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexclude_nan_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\training_utilities.py\u001b[0m in \u001b[0;36mvalidate_training_data_dict\u001b[1;34m(data_dict, automl_settings)\u001b[0m\n\u001b[0;32m    341\u001b[0m         validate_timeseries_training_data(automl_settings, X, y, X_valid, y_valid,\n\u001b[0;32m    342\u001b[0m                                           \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                                           x_raw_column_names)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\training_utilities.py\u001b[0m in \u001b[0;36mvalidate_timeseries_training_data\u001b[1;34m(automl_settings, X, y, X_valid, y_valid, sample_weight, sample_weight_valid, cv_splits_indices, x_raw_column_names)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         tsdf = _check_timeseries_input_and_get_tsdf(\n\u001b[1;32m--> 676\u001b[1;33m             X, y, x_raw_column_names, automl_settings, min_points, is_validation_data=False)\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[0mtsdf_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\training_utilities.py\u001b[0m in \u001b[0;36m_check_timeseries_input_and_get_tsdf\u001b[1;34m(X, y, x_raw_column_names, automl_settings, min_points, is_validation_data)\u001b[0m\n\u001b[0;32m    799\u001b[0m                 raise DataException(\n\u001b[0;32m    800\u001b[0m                     \u001b[1;34m\"Frequencies can not be inferred for grain [{}]. Please consider dropping that grain.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m                     \u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrain_name_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m                 )\n\u001b[0;32m    803\u001b[0m         \u001b[0m_check_tsdf_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies_grain_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDataException\u001b[0m: Frequencies can not be inferred for grain [('A00968', '33A1', '08BB')]. Please consider dropping that grain."
     ]
    }
   ],
   "source": [
    "time_series_settings = {\n",
    "    'time_column_name': time_colname,\n",
    "    'grain_column_names': grain_colnames,\n",
    "    'drop_column_names': [],\n",
    "    'max_horizon': 31\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are expected: our dataset has many intermittent series. For example, look at this series, focusing on the `SalesDate` column. Could *you* determine what its natural reporting frequency is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesDate</th>\n",
       "      <th>Item</th>\n",
       "      <th>Site</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79490</th>\n",
       "      <td>2012-01-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79491</th>\n",
       "      <td>2012-01-30</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79492</th>\n",
       "      <td>2012-02-02</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79493</th>\n",
       "      <td>2012-02-04</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79494</th>\n",
       "      <td>2012-02-17</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79495</th>\n",
       "      <td>2012-02-29</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79496</th>\n",
       "      <td>2012-03-07</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79497</th>\n",
       "      <td>2012-03-10</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79498</th>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79499</th>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79500</th>\n",
       "      <td>2012-03-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79501</th>\n",
       "      <td>2012-03-26</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79502</th>\n",
       "      <td>2012-04-25</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79503</th>\n",
       "      <td>2012-05-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79504</th>\n",
       "      <td>2012-05-17</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79505</th>\n",
       "      <td>2012-05-24</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79506</th>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79507</th>\n",
       "      <td>2012-06-16</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79508</th>\n",
       "      <td>2012-06-25</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79509</th>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79510</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>A0CA5B</td>\n",
       "      <td>1B3B</td>\n",
       "      <td>811B</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SalesDate    Item  Site Channel  Quantity\n",
       "79490  2012-01-24  A0CA5B  1B3B  811B   4.00     \n",
       "79491  2012-01-30  A0CA5B  1B3B  811B   9.00     \n",
       "79492  2012-02-02  A0CA5B  1B3B  811B   4.00     \n",
       "79493  2012-02-04  A0CA5B  1B3B  811B   3.00     \n",
       "79494  2012-02-17  A0CA5B  1B3B  811B   5.00     \n",
       "79495  2012-02-29  A0CA5B  1B3B  811B   4.00     \n",
       "79496  2012-03-07  A0CA5B  1B3B  811B   4.00     \n",
       "79497  2012-03-10  A0CA5B  1B3B  811B   7.00     \n",
       "79498  2012-03-14  A0CA5B  1B3B  811B   5.00     \n",
       "79499  2012-03-23  A0CA5B  1B3B  811B   5.00     \n",
       "79500  2012-03-24  A0CA5B  1B3B  811B   9.00     \n",
       "79501  2012-03-26  A0CA5B  1B3B  811B   5.00     \n",
       "79502  2012-04-25  A0CA5B  1B3B  811B   10.00    \n",
       "79503  2012-05-14  A0CA5B  1B3B  811B   8.00     \n",
       "79504  2012-05-17  A0CA5B  1B3B  811B   8.00     \n",
       "79505  2012-05-24  A0CA5B  1B3B  811B   6.00     \n",
       "79506  2012-06-14  A0CA5B  1B3B  811B   7.00     \n",
       "79507  2012-06-16  A0CA5B  1B3B  811B   6.00     \n",
       "79508  2012-06-25  A0CA5B  1B3B  811B   4.00     \n",
       "79509  2012-06-28  A0CA5B  1B3B  811B   8.00     \n",
       "79510  2012-07-20  A0CA5B  1B3B  811B   10.00    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQLdata[(SQLdata['Item'] == 'A0CA5B') & (SQLdata['Site'] == '1B3B') & (SQLdata['Channel'] == '811B')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 1: Filling in the implicit zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's help AutoML determine what the periodicity is by making the series explicitly daily. Since we are talking about sales, missing values correspond to zero quantity sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a nice helpful function that fills out the data frame with:\n",
    "# * zeros for the target column\n",
    "# * NaNs for the rest of the data\n",
    "# \n",
    "# Let's import it from out time series library in the repo.\n",
    "from tslib.ts_functions import fill_out_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1262885 values NOT including zeros\n",
      "Expecting 6063038 values including zeros\n",
      "Check these match... otherwise bad things happen to merge.\n",
      "['Item: object', 'Site: object', 'Channel: object', 'SalesDate: datetime64[ns]']\n",
      "['Item: object', 'Site: object', 'Channel: object', 'SalesDate: datetime64[ns]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6063038, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since frequency is hard to infer with intermittent data, we will tell the function explicitly\n",
    "# the frequency is daily\n",
    "complete = fill_out_with_zeros(SQLdata, time_colname, grain_colnames, target_colname, 'D')\n",
    "complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data engorged to 6M rows - all the days with no record now have explicit 0 sales. We call that a \"flat\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data comes out with the grain and time in a pandas MultiIndex\n",
    "flat_complete = complete.reset_index()\n",
    "flat_complete.to_csv(\"grocery_flat.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I split the data? It's about 5 years of data, 2012-01-01 to 2017-08-31.\n",
    "# It probably does not make sense to do more than one month ahead of daily predictions\n",
    "n_test_periods = 31 # days in August 2017\n",
    "\n",
    "# this will split off the last n VALUES from each grain. If the data has implicit zeros,\n",
    "# that means the test TIME INTERVAL may vary by grain.\n",
    "from tslib.ts_functions import split_last_n_by_grain\n",
    "\n",
    "# Do note the use of the parameter specifying the minimum grain length.\n",
    "# Because of how we pulled the data from SQL, the use is redundant.\n",
    "# But if your whole data fits in memory, you can filter out the short grains\n",
    "# in python, rather than at the source, using the splitting function\n",
    "df_train, df_test = split_last_n_by_grain(complete, \n",
    "                                          n_test_periods, \n",
    "                                          time_colname, grain_colnames, \n",
    "                                          min_grain_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5872729, 1)\n",
      "(190309, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how many datapoints we have. But do we really know where the train and test sets are situated in time?\n",
    "Let's look at their time extents. For ease of plotting, only two items (but with all their Site and Channel combinations) will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15948, 1)\n",
      "(1023, 1)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='d0d89539-40f9-4626-847f-b2906562804d'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smaller = complete.loc[(['A00968', 'A09D61'], slice(None), slice(None), slice(None)), :]\n",
    "small_train, small_test = split_last_n_by_grain(smaller, n_test_periods, time_colname, grain_colnames, min_grain_length=100)\n",
    "print(small_train.shape)\n",
    "print(small_test.shape)\n",
    "\n",
    "# this is for checking the time extent of the grains\n",
    "ranges_train = small_train.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_train.columns = ['begin', 'end']\n",
    "ranges_train_flat = ranges_train.reset_index()\n",
    "\n",
    "ranges_test = small_test.reset_index().groupby(grain_colnames).agg({'SalesDate': ['min', 'max']})\n",
    "ranges_test.columns = ['begin', 'end']\n",
    "ranges_test_flat = ranges_test.reset_index()\n",
    "\n",
    "# we should be able to plot that\n",
    "%matplotlib notebook\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 12]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hlines(range(len(ranges_train_flat)),xmin=ranges_train_flat['begin'].values,xmax=ranges_train_flat['end'].values)\n",
    "plt.hlines(range(len(ranges_test_flat)),xmin=ranges_test_flat['begin'].values,xmax=ranges_test_flat['end'].values, colors='r')\n",
    "ytick_tuples = list(zip(ranges_train_flat['Item'],ranges_train_flat['Site'], ranges_train_flat['Channel']))\n",
    "ytick_labels = list(map(lambda t : '-'.join(t), ytick_tuples))\n",
    "plt.yticks(range(len(ranges_test_flat)), ytick_labels)\n",
    "plt.show()\n",
    "\n",
    "# Improvements on this plot could include \n",
    "#    * plot little series instead of horizontal lines, \n",
    "#    * color-separating zero values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should ask ourselves whether the items whose histories do not reach the temporal end of the dataset are worth forecasting. Those items may have been discontinued, or they are still carried by the store but purchased infrequently. Since that is a question only a business owner can answer, let's leave them in for now. But have that little practicality in mind.\n",
    "\n",
    "Let's save the dataset we have in case we need to restart from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: flatten for writing and save\n",
    "flat_test = df_test.reset_index()\n",
    "flat_test.to_csv(\"grocery_flat_test.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train = df_train.reset_index()\n",
    "flat_train.to_csv(\"grocery_flat_trainvalid.csv.bz2\", compression='bz2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine what X and Y are - now with the zeros\n",
    "X_train = flat_train\n",
    "X_test = flat_test\n",
    "y_train = X_train.pop(target_colname).values\n",
    "y_test = X_test.pop(target_colname).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take some time to finish with 6M rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_eeca304c-d0cc-498d-a3cf-6907c5fc9b22\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating CV splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Type: Client\n",
      "Class: AutoMLPickleException\n",
      "Message: Uploading X failed with Pickle error \n",
      "Traceback:\n",
      "  File \"E:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\cache_store.py\", line 382, in _upload\n",
      "    self.pickler.dump(obj, path=path)\n",
      "  File \"E:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\pickler.py\", line 175, in dump\n",
      "    raise AutoMLPickleException(\"Pickle error {}\".format(e))\n",
      "\n"
     ]
    },
    {
     "ename": "AutoMLPickleException",
     "evalue": "Pickle error ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\pickler.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj, path)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                 \u001b[0mbio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAutoMLPickleException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-83691a40b82e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                              **time_series_settings)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlocal_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoml_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\core\\experiment.py\u001b[0m in \u001b[0;36msubmit\u001b[1;34m(self, config, tags, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0msubmit_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_experiment_submit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"submit config {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\automlconfig.py\u001b[0m in \u001b[0;36m_automl_static_submit\u001b[1;34m(automl_config_object, workspace, experiment_name, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'show_output'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoml_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mtracking_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoml_config_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, run_configuration, compute_target, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, data, label, columns, cv_splits_indices, show_output, existing_run)\u001b[0m\n\u001b[0;32m    299\u001b[0m             self._fit_local(X=X, y=y, sample_weight=sample_weight, X_valid=X_valid, y_valid=y_valid,\n\u001b[0;32m    300\u001b[0m                             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_splits_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                             existing_run=existing_run, sample_weight_valid=sample_weight_valid)\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_onnx_compatible_models\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_fit_local\u001b[1;34m(self, X, y, sample_weight, X_valid, y_valid, sample_weight_valid, data, label, columns, cv_splits_indices, existing_run)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mexperiment_observer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAzureExperimentObserver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_console_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m         \u001b[0mtransformed_data_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_transformed_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment_observer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         training_utilities.validate_data_splits(\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\train\\automl\\_azureautomlclient.py\u001b[0m in \u001b[0;36m_get_transformed_context\u001b[1;34m(self, experiment_observer)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mcache_store\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_store\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m                 \u001b[0mis_onnx_compatible\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoml_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_onnx_compatible_models\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m                 enable_feature_sweeping=self.automl_settings.enable_feature_sweeping)\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtransformed_data_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\data_transformation.py\u001b[0m in \u001b[0;36mtransform_data\u001b[1;34m(raw_data_context, preprocess, cache_store, is_onnx_compatible, logger, experiment_observer, enable_feature_sweeping)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mraw_data_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mraw_data_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mtransformed_data_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\data_context.py\u001b[0m in \u001b[0;36m_update_cache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickle_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_to_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_cache_with_featurized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturized_data_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturized_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\data_context.py\u001b[0m in \u001b[0;36m_add_to_cache\u001b[1;34m(self, k, value)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \"\"\"\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\cache_store.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, keys, values)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_debug_messages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Uploading key: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_or_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\cache_store.py\u001b[0m in \u001b[0;36m_upload\u001b[1;34m(self, key, obj)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_debug_messages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Uploaded key: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\master_0509\\lib\\site-packages\\azureml\\automl\\core\\_vendor\\automl\\client\\core\\common\\pickler.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, obj, path)\u001b[0m\n\u001b[0;32m    173\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_write_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_content\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAutoMLPickleException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pickle error {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAutoMLPickleException\u001b[0m: Pickle error "
     ]
    }
   ],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='r2_score',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # Here the X_train and y_train have\n",
    "                             y=y_train,   # explicit zeroes                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we are not testing here because we pretend AutoML had a memory error. The data we are using in the tutorial may not actually be too large as we are only using items beginning with 'A'. But with the full dataset, the AutoML run in the above cell did fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2: Splitting the datasets and composite models\n",
    "\n",
    "## (NOT your usual train/test splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint: restarting the processing from saved files\n",
    "# A good practice if you work with dataset sizes that crash python kernels\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'\n",
    "\n",
    "import pandas as pd\n",
    "flat_train = pd.read_csv(\"grocery_flat_trainvalid.csv.bz2\", compression='bz2', parse_dates=[time_colname])\n",
    "flat_test = pd.read_csv(\"grocery_flat_test.csv.bz2\", compression='bz2', parse_dates=[time_colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrived at this point because AutoML had an out-of-memory event. \n",
    "\n",
    "If the data is too large, we need to split the series into multiple chunks to be handled separately. The data amounts in the individual chunks should ideally be close to equal.\n",
    "\n",
    "It is best to split by the value of a specified column. It should be one of the grain columns - that way grains are guaranteed not to split across bins. \n",
    "\n",
    "'Item' seems like a good candidate column. How variable do the Item-based groups get in size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_train.groupby('Item').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the size is quite variable. Randomly assigning each item to a subset (chunk) would likely achieve approximately even chunks if the Items were about equal in data size, but we see the variation spans orders of magnitudes.\n",
    "\n",
    "Therefore, after grouping by column value, then approximately solve the bin packing problem with maximum volume equal to the max number of rows we are willing to have for one model. This will guarantee near-optimal packing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import split_into_chunks_by_size, split_into_chunks_by_groups\n",
    "    \n",
    "max_number_of_rows = 1 * 1000 * 1000;\n",
    "train_frames, indices = split_into_chunks_by_size(flat_train, 'Item', max_number_of_rows)\n",
    "# split test set using the same assignment of item to chunk (indices)\n",
    "test_frames = split_into_chunks_by_groups(flat_test, 'Item', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing: pickle the split datasets\n",
    "import pickle\n",
    "pickle.dump( (train_frames, test_frames, indices), open('split_datasets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and load them back after the lengthy procedure\n",
    "import pickle\n",
    "(train_frames, test_frames, indices) = pickle.load(open('split_datasets.pkl', 'rb'))\n",
    "\n",
    "grain_colnames = ['Item', 'Site', 'Channel']\n",
    "time_colname = 'SalesDate'\n",
    "target_colname = 'Quantity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we got decently sized chunks. Each chunk is a separate dataset that we will feed into AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "y_train = X_train.pop(target_colname).values\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first chunk has the expected size for the train and test dataset. Let's test whether we messed up the datasets in the process of splitting. Running it through AutoML will run the data checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,   # set the train to the NEW train set\n",
    "                             y=y_train,   # which is now the first chunk only                      \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import align_outputs\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Now that we were able to get a model, let's import the preferred and customary forecasting metrics and see how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslib.ts_functions import MAPE, SMAPE, MAE\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)\n",
    "mae = MAE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAE: %.2f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAPE, and even SMAPE metrics can be deceiving when dealing with time series that are often close to zero. They divide by small values and are quite volatile. This makes them a poor fit for such problems. \n",
    "\n",
    "MAE is a better metric for these cases because it avoids division. But be careful because MAE results are unnormalized and comparing them only makes sense in reference to a fixed test set. MAE places weight on series whose absolute value is large. This is often correct in business. Mis-forecasting the Microsoft intern program travel expenses is a lot smaller deal than mis-forecasting the Azure datacenter operations cost. Series trained towards MAE should be of similar magnitude. \n",
    "\n",
    "It is often helpful to stratify the series by the `ceil(log(mean(series)))` and train separate models which allows for more homogeneous training set magnitude-wise and will lead to better performance on smaller-magnitude series.\n",
    "\n",
    "But the best way to get a sense of how well a classifier is performing is to just plot the forecasts vs actuals. The mini-library has some plotting functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a time series plot for our difficult intermittent series\n",
    "from tslib.ts_plot_utils import plot_forecast\n",
    "\n",
    "\n",
    "#def plot_forecast(X_trainval, y_trainval,\n",
    "#                  X_test, y_test, y_pred,\n",
    "#                  target_column_name,\n",
    "#                  time_column_name,\n",
    "#                  actual_color='blue',\n",
    "#                  pred_color='green',\n",
    "#                  filter_dict = None)\n",
    "\n",
    "filt = {'Item' : 'A0CA5B',\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "plot_forecast(X_train, y_train, X_test, y_test, y_fcst,\n",
    "                  target_colname,\n",
    "                  time_colname,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a scatter plot of actual vs predicted\n",
    "\n",
    "%matplotlib notebook\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
    "test_actl = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
    "plt.legend((test_pred, test_actl), ('prediction', 'actual'), loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the models back together\n",
    "\n",
    "We split the data into small enough chunks that we can learn a model from each. But this is mildly inconvenient - we now have a collection of models and each of them can only process a subset of the data. We need to keep track of which models serves which `Item`s. To do this without writing awkward code on your side, we offer the general technique of *composite models*.\n",
    "\n",
    "In the forecasting case, we usually have a natural splitting column in the grain. It would be extraordinary to have a single time series (no grain) big enough to require this treatment.\n",
    "\n",
    "In the more general case of classification or regression, no good categorical splitting column may exist in the data. You can still split the data, and all the models you learn will be \"universal\" - capable of classifying any instance. You will lose some predictive power, because you learned from smaller datasets. But you can get it back by running the example through `predict()` of *all* the models and ensembling the predictions (just take a simple majority vote or average).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompositeModel:    \n",
    "    \n",
    "    def __init__(self, split_column, target_column):\n",
    "        self._split_column = split_column\n",
    "        self._target_column = target_column\n",
    "        self._item_run_map = dict()\n",
    "        self._item_model_map = dict()\n",
    "        self._model_impls = dict()\n",
    "        self._indices = dict()\n",
    "    \n",
    "\n",
    "    # todo: redo this so that this class splits its own training data\n",
    "    # on the splitcolumn, ideally in sequence, rather than materializing\n",
    "    # the whole dataset again in memory as a sequence of chunks. \n",
    "    \n",
    "    def fit(self, train_frames, indices):    \n",
    "        \n",
    "        self._indices = indices\n",
    "            \n",
    "        # TODO: this is hard-wired and overwrites outer scope        \n",
    "        time_series_settings = {\n",
    "            'time_column_name': time_colname,\n",
    "            'grain_column_names': grain_colnames,\n",
    "            'drop_column_names': [],\n",
    "            'max_horizon': 31\n",
    "        }\n",
    "        \n",
    "        for idx, Xy in enumerate(train_frames):\n",
    "            \n",
    "            if len(Xy) == 0:\n",
    "                print('Warning: found a zero-length frame at index ' + str(idx))\n",
    "                continue\n",
    "            \n",
    "            X_train = Xy.copy()            \n",
    "            y_train = X_train.pop(self._target_column).values                        \n",
    "          \n",
    "            automl_config = AutoMLConfig(task='forecasting',\n",
    "                                debug_log='automl-grocery.log',\n",
    "                                primary_metric='normalized_root_mean_squared_error',\n",
    "                                iterations=5,\n",
    "                                X=X_train,\n",
    "                                y=y_train,                             \n",
    "                                n_cross_validations=3,\n",
    "                                enable_ensembling=False,\n",
    "                                path=project_folder,\n",
    "                                verbosity=logging.INFO,    \n",
    "                                **time_series_settings)\n",
    "        \n",
    "            # get the model and metadata\n",
    "            local_run = experiment.submit(automl_config, show_output=True) # Parent run \n",
    "            best_run, fitted_pipeline = local_run.get_output()             # Favorite child\n",
    "            model_id = best_run.id\n",
    "            print('Learned model ' + str(model_id))  # this is not working - needs a different ID\n",
    "        \n",
    "            # record the model for item\n",
    "            self._model_impls[model_id] = fitted_pipeline\n",
    "            for idx, item in enumerate(self._indices[idx]):\n",
    "                self._item_model_map[item] = model_id\n",
    "                self._item_run_map[item] = best_run.id\n",
    "                \n",
    "                \n",
    "    def forecast(self, X_test, y_test):\n",
    "        \n",
    "        # split X and y together by splitcolumn\n",
    "        X_copy = X_test.copy()\n",
    "        X_copy['__automl_target_column'] = y_test\n",
    "        chunks = split_into_chunks_by_groups(X_copy, self._split_column, self._indices)\n",
    "        \n",
    "        ys = []\n",
    "        X_transes = []\n",
    "        for chunk in chunks:\n",
    "            \n",
    "            # skip potentially empty splits\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Look up the right model. It should be the same model \n",
    "            # for the whole chunk by construction\n",
    "            item = chunk.loc[chunk.index[0], self._split_column]\n",
    "            modelid = self._item_model_map[item]\n",
    "            print('Using model ' + str(modelid))\n",
    "            model = self._model_impls[modelid]\n",
    "            \n",
    "            paranoid = True\n",
    "            if paranoid:\n",
    "                for item2 in pd.unique(chunk[self._split_column]):\n",
    "                    assert(\n",
    "                           (item2 in self._item_model_map.keys()) and \n",
    "                           (self._item_model_map[item2] == modelid),\n",
    "                           'Item ' + str(item2) + ' is not mapped to the same model ' + str(modelid) + ' as ' + str(item)\n",
    "                    )\n",
    "                           \n",
    "            y_chunk = chunk.pop('__automl_target_column').values\n",
    "            y_pred, X_trans = model.forecast(chunk, y_chunk)\n",
    "            ys.append(y_chunk)\n",
    "            X_transes.append(X_trans)\n",
    "            \n",
    "        return np.concatenate(ys), pd.concat(X_transes)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = CompositeModel(split_column = 'Item', target_column = 'Quantity')\n",
    "cm.fit(train_frames, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forecasts on one test dataframe\n",
    "X_test0 = test_frames[0].copy()\n",
    "y_test0 = X_test0.pop(target_colname).values\n",
    "y_test0.fill(np.nan)\n",
    "r0 = cm.forecast(X_test0, y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full test set, actuals, and query\n",
    "\n",
    "# OK to copy the smaller test frames\n",
    "ys = [ X[target_colname].values for X in test_frames ]\n",
    "Xs = [ X.copy().drop(columns=[target_colname]) for X in test_frames]\n",
    "X_test = pd.concat(Xs)\n",
    "y_test = np.concatenate(ys)\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on all chunks\n",
    "y_fcst, X_trans = cm.forecast(X_test,y_test)\n",
    "df_all = align_outputs(y_fcst, X_trans, X_test, y_test)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.2f' % mae)\n",
    "mape = MAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('MAPE: %.2f' % mape)\n",
    "smape = SMAPE(df_all[target_column_name], df_all['predicted'])\n",
    "print('SMAPE: %.2f' % smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 3: Target transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is extremely challenging due to the abundance of zero values. Also, the forecasts sometimes venture below zero, which will not make sense to business customers. In those cases, it often helps to transform the target value with something like a log-transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_transform(nparr):\n",
    "    return nparr.apply(lambda x: math.log(x+1))\n",
    "    \n",
    "\n",
    "def log_inverse_transform(nparr):\n",
    "    \"\"\"\n",
    "    With zero truncation so not a perfect inverse\n",
    "    \"\"\"\n",
    "    return nparr.apply(lambda y: max(0, math.exp(y) - 1))\n",
    "    \n",
    "# For example, \n",
    "log_inverse_transform(log_transform(y_train[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "model_valid_for = indices[idx]\n",
    "X_train = train_frames[idx].copy()\n",
    "X_test = test_frames[idx].copy()\n",
    "\n",
    "# NOTE log transform\n",
    "y_train = log_transform(X_train.pop(target_colname).values)\n",
    "y_test = X_test.pop(target_colname).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log='automl-grocery.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             X=X_train,\n",
    "                             y=y_train,                             \n",
    "                             n_cross_validations=2,\n",
    "                             enable_ensembling=False,\n",
    "                             path=project_folder,\n",
    "                             verbosity=logging.INFO,    \n",
    "                             **time_series_settings)\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_pipeline = local_run.get_output()\n",
    "fitted_pipeline.steps\n",
    "y_query = y_test.copy()\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_pipeline.forecast(X_test, y_query)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = {'Item' : 'A0CA5B',\n",
    "        'Site' : '1B3B',\n",
    "        'Channel' : '811B' }\n",
    "\n",
    "# TESTME: do we need to plot from the aligned data frame?\n",
    "plot_forecast(X_train, y_train, X_test, y_test, \n",
    "                  inverse_log_transform(y_fcst),       # NOTE inverse transform\n",
    "                  target_colname,\n",
    "                  time_colname,\n",
    "                  filter_dict = filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveat 4: A note on pandas indexes\n",
    "\n",
    "One problem that often occurs with dataframes arising from filtering is that *level names* are retained in the index when the values are *removed*. This can create empty groups when grouping by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem is pandas version dependent (TODO: which versions?)\n",
    "print(pd.__version__)\n",
    "\n",
    "ex_df = pd.DataFrame({'Item' : ['A', 'A', 'B', 'B', 'C'], \n",
    "                       'Day' : ['Mon', 'Tue', 'Mon', 'Tue', 'Mon'],\n",
    "                       'Sales' : [1, 2, 3, 4, 5]})\n",
    "ex_df.set_index(['Item'], inplace=True)\n",
    "ex_df = ex_df[ex_df['Day'] == 'Tue']  # filter down to Tue, leaving out 'C'\n",
    "# ex_df.index.get_level(level='Item')\n",
    "ex_df.groupby(level='Item').sum()\n",
    "\n",
    "# TODO: make this problem actually show up with the wrong pandas version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest fix is to rebuild the dataframe from csv\n",
    "ex_df.reset_index().to_csv('temp_ex_df.csv.bz2', index=False)\n",
    "ex_df = pd.read_csv('temp_ex_df.csv.bz2')\n",
    "\n",
    "# or use StringIO if you having multiple data copies in memory is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes hard-to-understand errors, so let's make sure it didn't happen to us, checking ALL the data chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty levels\n",
    "\n",
    "def empty_grains(df, grain_colnames):\n",
    "    \"\"\"\"\n",
    "    returns the list of empty grains\n",
    "    \"\"\"\n",
    "    empties = df.groupby(grain_colnames).filter(lambda g: len(g) == 0)\n",
    "    return empties\n",
    "\n",
    "frames_with_empties = [empty_grains(df, grain_colnames).shape[0] > 0 for df in train_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(frames_with_empties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is *different* from checking whether some of the training data frames are actually empty. That would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty dataframes in train_frames\n",
    "[idx for idx, df in enumerate(train_frames) if len(df)==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions: What is missing here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise does not address a few things. But you are now equipped to build them on your own.\n",
    "* Deployment of composite models\n",
    " * The deployment procedure will need to register all the models in the dictionary.\n",
    " * The indices dictionary will need to be deployed to the docker image in pickled form.\n",
    " * The template script `init` function will load the indices into memory.\n",
    " * The template script `run` function will need lazy loading of the models from the workspace.\n",
    "* Those orphaned short series.\n",
    " * We left a number of series behind when we decided they were \"too short for ML\". That is quite some hubris;\n",
    "   chances are the business owners do need those forecasts. \n",
    "  * We should therefore create another wraparound model which will provide forecasts for them. \n",
    "    High accuracies cannot be expected from such data, and a simple model will do. \n",
    "    In the pattern of the composite model, we could keep a dictionary of retained parameters\n",
    "    of say, an exponential smoothing model, for each grain in the dataset.\n",
    "* Info schedule issues\n",
    " * TODO: write the function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (master_0509)",
   "language": "python",
   "name": "master_0509"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
